{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the fun part, machine learning!\n",
    "\n",
    "The `seaborn` package is a wrapper around `matplotlib` but in this case I'm just exploiting the built in data sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Acquire data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris data set is the 'hello world' of machine learning.  It's a very simple data set with data about 150 iris flowers.  There are four measurements and an associated species.  There are 3 species, with 50 instances of each in the data set.  Our task is to create a 'classifier' which, when given values for the measurements, will (hopefully) correctly predict the species to which that flower belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Prepare Data**\n",
    "\n",
    "Scikit-learn (`sklearn`) doesn't like text values so I'm mapping a numeric value to each species.\n",
    "\n",
    "*Machine-learning afficiandos: Yes I am using ordinal values here.  However, it's a small example and doesn't really make a difference in the results.  Also, I get to save time by avoiding the explanation of one-hot encoding.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_codes = {\n",
    "    'setosa': 0,\n",
    "    'versicolor': 1,\n",
    "    'virginica': 2\n",
    "}\n",
    "\n",
    "iris['target'] = [species_codes[s] for s in iris.species]\n",
    "\n",
    "iris.iloc[[0,1,50,51,100,101]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Split data**\n",
    "\n",
    "I'm going to split the data set into two parts, a training data set and a test data set.  The training data set will be used to generate a *model* which will represent the 'knowledge' that has been dervied from analyzing the data.  Then the test set will be passed into the model which will predict a species for each instance.  The predictions of the model will be compared to the values in the test data set to evaluate the accuracy of the model.\n",
    "\n",
    "The function I am using for this comes from scikit-learn.  It requires that the features (independent variables) be separated from the targets (dependent variables).  This is easy with `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = iris[iris.columns[:4]]\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_test_split` function accepts a keyword argument with the size of the test data set.  In this case I'm using 50% because of small data set size.  This is to intentionally introduce some errors.  In practice 20-30% would suffice but with the Iris data set it trains the data set 'perfectly'.\n",
    "\n",
    "The function returns four values: the training and test sets for the features and the same for the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice something interesting about these new data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indicies are randomized.  This is to attempt to get a roughly equal distribution of the targets in both the training and test data sets.  What would happen if we selected the first 50% of the Iris data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter(y[np.arange(75)].values)\n",
    "c.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the rows are grouped by species, the training data set would include no virginica and the test data set would include no setosa.  This would undoubtedly result in an unacceptable model.\n",
    "\n",
    "Randomly rearranging the indicies is better but still not as even as the `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "idx = np.arange(150)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "c = Counter(y[idx[:75]].values)\n",
    "c.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again this is more obvious with a larger data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(test_y.values)\n",
    "c.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Train model**\n",
    "\n",
    "Here is where the heavy lifting beings.  However, it's the simplest part of the code.  This is the power of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: Test model**\n",
    "\n",
    "Using the `iris_model`, use each row of the test data set to pass features to the `predict` method. (The `reshape` method just transforms the values from a row to a column which scikit-learn wants, don't worry about it.)  The result will be a list of values, one for each target.  Since there is only one target - species - there will only be one value.  Compare it to the known value in the test data set and keep track of the number of correct predicitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = 0\n",
    "\n",
    "for idx in test_X.index:\n",
    "    prediction = iris_model.predict(test_X.loc[idx].values.reshape(1, -1))\n",
    "    if test_y.loc[idx] == prediction[0]: predictions += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7: Evaluate model**\n",
    "\n",
    "The simplest measure of accuracy is the ratio of correct predictions to total predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = predictions / len(test_X)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'{}%'.format(round(accuracy * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
